import json

# Define an Agent class that runs a structured Thought -> Action -> Observation -> Final loop
class Agent:
  # Initialization function
  def __init__(self, client, tools_list, role, backstory):
    """
    ---------------------------------------------------------------------
    -------------------------START OF FUNCTION---------------------------
    ---------------------------------------------------------------------
    """
    
    self.client = client # Save the LLM client
    self.tools = {} # Contains tool descriptions
    self.messages: list = [] # Store the conversation history
    self.complete_prompt = f"""
    ### Who you are: 
    {role}

    ### Your description as provided: 
    {backstory}

    Given a user's task, you must work in a process following the loop of Thought, Action, Observation, to produce what the user wants. At the end of the loop, you output a final answer.

    You must understand the task the user is asking of you, and generate output that perfectly fits their request in accordance with your abilities.
    """ # Represents final system prompt

    tool_description = """
    ### Below are your available actions

    """

    for tool_name, tool_obj, in tools_list.items():
      description_tool = f"""
      Action name: ${tool_name}
      Action description ${tool_obj["description"]}
      Action parameters and their descriptions: ${tool_obj["parameters_descriptions"]}

      """
      tool_description += description_tool
      self.tools[tool_name] = tool_obj["function"]
    
    self.complete_prompt += """
    {tool_description}

    -----

    ### Strict requirements for every output:

    1. **JSON Standardized Output**: Every single response must be a JSON object in the following format:

    {
      "stage": "thought" | "action" | "observation" | "final",
      "action": { "action_name": {"param1": "value1", "param2": "value2"} } | null,
      "thought": "..." | null,
      "observation": "..." | null,
      "final_output": "..." | null
    }

    - For stages other than the current stage, set the respective field to `null`.  
    - `stage` must indicate the current stage.  
    - `action` is only non-null in the Action stage.  
    - `thought` is only non-null in the Thought stage.  
    - `observation` is only non-null in the Observation stage.  
    - `final_output` is only non-null in the Final stage.
    - The "final_output" attribute will contain a Markdown parsable output that would be presented to the user as the final response to their request

    2. **One Stage Per Response**: You must output exactly one stage per response and then stop. 
      Wait for the next call to continue the process.

    3. **Tool Usage**: Try to use the tools available to you. As a last resort if you think that the tools would not be helpful for your thought, you may proceed with your own knowledge. Only use the tools listed above. Do not invent new tools.

    ---

    ### EXAMPLE:

    User input: "I want to make a futuristic landing page for my hackathon"

    1. Thought Stage:

    {
      "stage": "thought",
      "action": null,
      "thought": "The user wants a futuristic-themed landing page for a hackathon. I should first generate the HTML structure that fits this description.",
      "observation": null,
      "final_output": null
    }

    2. Action Stage (after Thought):

    {
      "stage": "action",
      "action": {"generate_html": {"prompt": "Create a futuristic landing page for a hackathon with a hero section, event description, and a call-to-action button saying 'Register Now'"}},
      "thought": null,
      "observation": null,
      "final_output": null
    }

    3. Observation Stage (after Action):

    {
      "stage": "observation",
      "action": null,
      "thought": null,
      "observation": "<HTML output generated by generate_html tool>",
      "final_output": null
    }

    4. Thought Stage (after Observation):

    {
      "stage": "thought",
      "action": null,
      "thought": "Now that I have the HTML, I need to style it with a futuristic theme: dark background, neon-colored text and buttons, and modern fonts.",
      "observation": null,
      "final_output": null
    }

    5. Action Stage:

    {
      "stage": "action",
      "action": {"generate_css": {"html_code": "<HTML output from previous step>", "prompt_design": "Style the page with a dark background, neon text and buttons, futuristic fonts, and a sleek, modern look for a hackathon landing page"}},
      "thought": null,
      "observation": null,
      "final_output": null
    }

    6. Observation Stage:

    {
      "stage": "observation",
      "action": null,
      "thought": null,
      "observation": "<CSS output generated by generate_css tool>",
      "final_output": null
    }

    7. Final Stage:

    {
      "stage": "final",
      "action": null,
      "thought": null,
      "observation": null,
      "final_output": "<FINAL GENERATED MARKDOWN CONTENT HERE>" 
    }
    """

    # Add the system prompt as the first message if provided
    if self.complete_prompt:
      self.messages.append({"role": "system", "content": self.complete_prompt})

    """
    ---------------------------------------------------------------------
    -------------------------END OF FUNCTION-----------------------------
    ---------------------------------------------------------------------
    """

  # Wrapper to call the LLM with the conversation so far
  def call_LLM_here(self, promptCallLLM):
    """
    ---------------------------------------------------------------------
    -------------------------START OF FUNCTION---------------------------
    ---------------------------------------------------------------------
    """
    response = self.client.responses.create(
      model="gpt-5",       # Use GPT-5 model
      input=promptCallLLM  # Full conversation history
    )
    outp = response.output_text  # Extract the text output
  
    """
    ---------------------------------------------------------------------
    -------------------------END OF FUNCTION-----------------------------
    ---------------------------------------------------------------------
    """
    return outp

  # Core execution loop: keeps calling the LLM until "final" stage is reached
  def execute_loop(self):
    """
    ---------------------------------------------------------------------
    -------------------------START OF FUNCTION---------------------------
    ---------------------------------------------------------------------
    """
    final_output = {}
    print()
    while True:
      # Call LLM with current conversation history
      llm_text = self.call_LLM_here(self.messages)

      # Parse the LLM JSON response
      llm_output = json.loads(llm_text)
      stageCurr = llm_output["stage"]
      # Handle "thought" and "observation" stages -> just log and continue
      if stageCurr == "thought" or stageCurr == "observation":
        print(f"------------- STAGE: {stageCurr} -------------")
        if stageCurr=="thought": print(f"Thought: {llm_output["thought"]}")
        elif stageCurr=="observation": print(f"Observation: {llm_output["observation"]}")
        print(f"----------------------------------------------")
        print()
        print()
        self.messages.append({"role": "assistant", "content": 
        f"""
        {llm_text}
        """})
      
      # Handle "action" stage -> call the tool and append the tool result
      elif stageCurr == "action":
        print(f"------------- STAGE: ACTION -------------")
        print(f"Tool: {list(llm_output["action"].keys())[0]}")
        print(f"----------------------------------------------")
        print()
        print()
        tool_output = self.run_tool(llm_output["action"])
        
        observation_json = {
            "stage": "observation",
            "action": None,
            "thought": None,
            "observation": tool_output,
            "final_output": None
        }
        self.messages.append({"role": "assistant", "content": 
        f"""
        {json.dumps(observation_json)}
        """})
      
      # Handle "final" stage -> break out of the loop and return the result
      elif stageCurr == "final":
        print(f"------------- STAGE: FINAL OUTPUT -------------")
        print(f"----------------------------------------------")
        print()
        print()
        final_output = llm_output["final_output"]
        break
    """
    ---------------------------------------------------------------------
    -------------------------END OF FUNCTION---------------------------
    ---------------------------------------------------------------------
    """
    return final_output

  # Executes a given tool call
  def run_tool(self, tool_call):
    for tool_name, params in tool_call.items():
        if tool_name not in self.tools:
            raise ValueError(f"Tool '{tool_name}' is not available.")
        
        func = self.tools[tool_name]  # Get the actual Python function
        return func(**params)         # Call it with the provided parameters
        
  # Main entrypoint: run the loop and export results to Markdown
  def __call__(self, user_input):
    if user_input:
      self.messages.append({"role": "user", "content": user_input})
    
    final_output = self.execute_loop()
    
    # Export result to markdown file
    with open("output.md", "w") as f:
      f.write(final_output)

    print("Markdown exported!")
